---
title: "Forecasting Retail Sales of Beer, Wine, and Liquor Stores"
author: "Nathan Kim"
date: "Fall Quarter 2021"
output:
  html_document: default
  pdf_document: default
---

```{r, echo=F, message=F}
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

```{r, echo=F}
rm(list=ls(all=TRUE))
```

```{r, warning=F, message=F}
library(fredr)
library(fpp2)
library(tseries)
library(forecast)
library(rstan)
library(prophet)
library(ggplot2)
library(MLmetrics)
library(tseries)
library(strucchange)

# Insert own API key from FRED 
fredr_set_key(fred_key)
```

# Introduction 

This time series analysis focuses on monthly retail sales of beer, wine, and liquor stores in the United States from January 1st, 1992 to October 1st, 2021. Sales are measured in millions of nominal dollars and are not seasonally adjusted. For the purposes of modeling, sales were log transformed. Upon visual inspection, the data shows a positive trend strong seasonality. For alcohol sales, we expect to see a spike during the months containing holidays such as New Year’s Eve. Interestingly, retail sales for alcohol appears to be somewhat “recession proof”. When we look at retail sales as a whole, we see a dip in sales caused by recessions. However, retail sales specifically for alcohol does not seem to experience these same interruptions. In fact, alcohol sales has spiked during the COVID-19 Recession. 

# Data Inspection 

```{r}
sales    <- fredr(series_id="MRTSSM4453USN")
sales_ts <- ts(log(sales$value), start=1992, frequency=12) 

any(is.na(sales_ts))
```

```{r, warning=F}
adf.test(sales_ts)
```
The ADF test tells us that we reject the null hypothesis of non-stationarity at the 5% significance level. 

```{r}
autoplot(sales_ts) +
  ylab("Log(Sales)") + 
  ggtitle("Monthly Retail Sales of Beer, Wine, and Liquor Stores")
autoplot(stl(sales_ts, s.window="periodic"), main="STL Decomposition")
ggtsdisplay(sales_ts, main="Series with ACF and PACF") 
```

After taking the log of the series, seasonality has become more constant. The decay in spikes in the ACF and the few significant spikes in the PACF suggest that alcohol sales follow an AR(p) process. 


```{r}
n     <- round((2/3)*length(sales_ts)) 

# 2/3 train set 
train <- ts(sales_ts[1:n], start=1992, frequency=12) 

# 1/3 test set 
test  <- ts(sales_ts[n+1:length(sales_ts)], start=2011.917, frequency=12)
test  <- na.remove(test)
```

He we split the time series data into train and test sets to allow for backtesting. 

# Comparing Models

## ARIMA 

```{r}
f_arima <- forecast(auto.arima(train), h=length(sales_ts)-n)

autoplot(train, series="Train") +
  autolayer(test, series="Test") + 
  autolayer(f_arima$mean, series="ARIMA") + 
  ggtitle("Forecasting with ARIMA") + 
  ylab("Log(Sales)") + 
  labs(colour="")
```

```{r}
arima_recur <- c() 
for (i in n:length(sales_ts)) { 
  predict     <- forecast(auto.arima(ts(sales_ts[1:i], frequency=12),
                                     method="CSS"), h=12) 
  actual      <- sales_ts[(i+1):(i+12)] 
  arima_recur <- cbind(arima_recur, MAPE(predict$mean, actual))
}

arima_recur    <- na.omit(as.data.frame(t(arima_recur)))
h              <- c(1:nrow(arima_recur))
arima_recur_df <- cbind.data.frame(h, arima_recur)
```

```{r}
arima_roll <- c()
for (i in 0:(length(sales_ts)-n)) { 
  predict    <- forecast(auto.arima(ts(sales_ts[(1+i):(n+i)],frequency=12), 
                                    method="CSS"), h=12) 
  actual     <- sales_ts[(n+i+1):(n+i+12)]
  arima_roll <- cbind(arima_roll, MAPE(predict$mean, actual))
}

arima_roll    <- na.omit(as.data.frame(t(arima_roll)))
h             <- c(1:nrow(arima_roll))
arima_roll_df <- cbind.data.frame(h, arima_roll) 
```

```{r}
mean(arima_recur$V1)
mean(arima_roll$V1)
```

## ETS 

```{r}

# ETS 
f_ets <- forecast(ets(train), h=length(sales_ts)-n)

autoplot(train, series="Train") +
  autolayer(test, series="Test") + 
  autolayer(f_ets$mean, series="ETS") + 
  ggtitle("Forecasting with ETS") + 
  ylab("Log(Sales)") + 
  labs(colour="")  
```

```{r}
ets_recur <- c() 
for (i in n:length(sales_ts)) { 
  predict   <- forecast(ets(ts(sales_ts[1:i], frequency=12)), h=12) 
  actual    <- sales_ts[(i+1):(i+12)] 
  ets_recur <- cbind(ets_recur, MAPE(predict$mean, actual))
}

ets_recur    <- na.omit(as.data.frame(t(ets_recur)))
h            <- c(1:nrow(ets_recur))
ets_recur_df <- cbind.data.frame(h, ets_recur) 
```

```{r}
ets_roll <- c()
for (i in 0:(length(sales_ts)-n)) { 
  predict  <- forecast(ets(ts(sales_ts[(1+i):(n+i)], frequency=12)), h=12) 
  actual   <- sales_ts[(n+i+1):(n+i+12)]
  ets_roll <- cbind(ets_roll, MAPE(predict$mean, actual))
}

ets_roll    <- na.omit(as.data.frame(t(ets_roll)))
h           <- c(1:nrow(ets_roll))
ets_roll_df <- cbind.data.frame(h, ets_roll) 
```

```{r}
mean(ets_recur$V1)
mean(ets_roll$V1)
```

# Holt-Winters 

```{r}
f_hw <- forecast(HoltWinters(train), h=length(sales_ts)-n)
autoplot(train, series="Train") + 
  autolayer(test, series="Test") + 
  autolayer(f_hw$mean, series="HW") + 
  labs(colour="") + 
  ggtitle("Forecasting with Holt Winters") +
  ylab("Log(Sales)")
```

```{r}
hw_recur <- c() 
for (i in n:length(sales_ts)) { 
  predict  <- forecast(hw(ts(sales_ts[1:i], frequency=12)), h=12) 
  actual   <- sales_ts[(i+1):(i+12)] 
  hw_recur <- cbind(hw_recur, MAPE(predict$mean, actual))
}

hw_recur    <- na.omit(as.data.frame(t(hw_recur)))
h           <- c(1:nrow(hw_recur))
hw_recur_df <- cbind.data.frame(h, hw_recur) 
```

```{r}
hw_roll <- c()
for (i in 0:(length(sales_ts)-n)) { 
  predict <- forecast(hw(ts(sales_ts[(1+i):(n+i)], frequency=12)), h=12) 
  actual  <- sales_ts[(n+i+1):(n+i+12)]
  hw_roll <- cbind(hw_roll, MAPE(predict$mean, actual))
}

hw_roll    <- na.omit(as.data.frame(t(hw_roll)))
h          <- c(1:nrow(hw_roll))
hw_roll_df <- cbind.data.frame(h, hw_roll) 
```

```{r}
mean(hw_recur$V1)
mean(hw_roll$V1)
```

## Facebook Prophet 

```{r, message=F}
sales_df           <- cbind.data.frame(sales$date, log(sales$value))
colnames(sales_df) <- c("ds", "y")

train_prophet <- sales_df[1:n,]
test_prophet  <- sales_df[n+1:nrow(sales_df),]

prophet      <- prophet(train_prophet)
future       <- make_future_dataframe(prophet, periods=nrow(sales_df)-n, 
                                      freq='month')
f_prophet    <- predict(prophet, future)
f_prophet_ts <- na.remove(ts(f_prophet$yhat[n+1:nrow(f_prophet)], 
                             start=2011.917, frequency=12))

autoplot(train, series="Train") + 
  autolayer(test, series="Test") + 
  autolayer(f_prophet_ts, series="FBP") + 
  ggtitle("Forecasts from Facebook Prophet") + 
  ylab("Log(Sales)") + 
  labs(colour="")
```

```{r, message=F}
fbp_recur <- c() 
for (i in n:length(sales_ts)) { 
  df        <- sales_df[1:i,] 
  model     <- prophet(df)
  future    <- make_future_dataframe(model, periods=12, freq='month')
  f_prophet <- predict(model, future)
  predict   <- f_prophet$yhat[(nrow(f_prophet)-11):nrow(f_prophet)]
  actual    <- sales_ts[(i+1):(i+12)] 
  fbp_recur <- cbind(fbp_recur, MAPE(predict, actual))
}

fbp_recur    <- na.omit(as.data.frame(t(fbp_recur)))
h            <- c(1:nrow(fbp_recur))
fbp_recur_df <- cbind.data.frame(h, fbp_recur) 
```

```{r, message=F}
fbp_roll <- c()
for (i in 0:(length(sales_ts)-n)) { # i in 0:119
  df        <- sales_df[(1+i):(n+i),] # train: 1 + i : 239 + i
  model     <- prophet(df)
  future    <- make_future_dataframe(model, periods=12, freq='month')
  f_prophet <- predict(model, future)
  predict   <- f_prophet$yhat[(nrow(f_prophet)-11):nrow(f_prophet)]
  actual    <- sales_ts[(n+i+1):(n+i+12)]
  fbp_roll  <- cbind(fbp_roll, MAPE(predict, actual))
}

fbp_roll    <- na.omit(as.data.frame(t(fbp_roll)))
h           <- c(1:nrow(fbp_roll))
fbp_roll_df <- cbind.data.frame(h, fbp_roll) 
```

```{r}
mean(fbp_recur$V1)
mean(fbp_roll$V1)
```

# Combined Forecast 

We will now combine the forecasts by taking the average of the results from the 4 models above. 

```{r, message=F}
prophet   <- prophet(train_prophet)
future    <- make_future_dataframe(prophet, periods=nrow(sales_df)-n, 
                                   freq='month')
f_prophet <- predict(prophet, future)
fbp       <- ts(na.remove(f_prophet$yhat[n+1:nrow(f_prophet)]), start=2012,
                frequency=12)

arima  <- forecast(auto.arima(train), h=length(sales_ts)-n)
ets    <- forecast(ets(train), h=length(sales_ts)-n)
holtw  <- forecast(HoltWinters(train), h=length(sales_ts)-n)
comb   <- (fbp + arima[["mean"]] + ets[["mean"]] + holtw[["mean"]])/4


autoplot(sales_ts) + 
  autolayer(arima$mean, series="ARIMA") + 
  autolayer(ets$mean, series="ETS") + 
  autolayer(holtw$mean, series="HW") +
  autolayer(fbp, series="FBP") +
  autolayer(comb, series="Combined") +
  ylab("Log(Sales)") + 
  labs(color="Forecasts") +
  ggtitle("Forecasting Using Combined Model") + 
  xlim(2008,2022) + 
  ylim(7.75, 9)
```

```{r, message=F}
comb_recur <- c() 
for (i in n:length(sales_ts)) {
  df1 <- ts(sales_ts[1:i], frequency=12)
  f1  <- forecast(auto.arima(df1, method="CSS"), h=12)
  f2  <- forecast(ets(df1), h=12)
  f3  <- forecast(hw(df1), h=12)
  
  df2       <- sales_df[1:i,]
  m4        <- prophet(df2)
  future    <- make_future_dataframe(m4, periods=12, freq='month')
  f_prophet <- predict(m4, future)
  f4        <- f_prophet$yhat[(nrow(f_prophet)-11):nrow(f_prophet)]
  
  f1 <- as.vector(f1$mean)
  f2 <- as.vector(f2$mean)
  f3 <- as.vector(f3$mean)
  f4 <- as.vector(f4)
  
  predict <- (f1 + f2 + f3 + f4)/4
  actual  <- sales_ts[(i+1):(i+12)] 
  
  comb_recur <- cbind(comb_recur, MAPE(predict, actual))
}

comb_recur    <- na.omit(as.data.frame(t(comb_recur)))
h             <- c(1:nrow(comb_recur))
comb_recur_df <- cbind.data.frame(h, comb_recur) 
```

```{r, message=F}

comb_roll <- c()
for (i in 0:(length(sales_ts)-n)) { 
  df1     <- ts(sales_ts[(1+i):(n+i)], frequency=12)
  f1      <- forecast(auto.arima(df1, method="CSS"), h=12)
  f2      <- forecast(ets(df1), h=12)
  f3      <- forecast(hw(df1), h=12)
  
  df2       <- sales_df[(1+i):(n+i),]
  m4        <- prophet(df2)
  future    <- make_future_dataframe(m4, periods=12, freq='month')
  f_prophet <- predict(m4, future)
  f4        <- f_prophet$yhat[(nrow(f_prophet)-11):nrow(f_prophet)]
  
  f1 <- as.vector(f1$mean)
  f2 <- as.vector(f2$mean)
  f3 <- as.vector(f3$mean)
  f4 <- as.vector(f4)
  
  predict   <- (f1 + f2 + f3 + f4)/4
  actual    <- sales_ts[(n+i+1):(n+i+12)] 
  comb_roll <- cbind(comb_roll, MAPE(predict, actual))
 
}

comb_roll    <- na.omit(as.data.frame(t(comb_roll)))
h            <- c(1:nrow(comb_roll))
comb_roll_df <- cbind.data.frame(h, comb_roll)
```

```{r}
mean(comb_recur$V1)
mean(comb_roll$V1)
```
On average, we can see that the combined model performs the best in terms of MAPE. 

# Results of Backtesting

```{r}
mape1 <- cbind.data.frame(h, arima_recur$V1, ets_recur$V1, 
                         hw_recur$V1, fbp_recur$V1, comb_recur$V1)
colnames(mape1) <- c("Iteration", "ARIMA", "ETS", "HW", "FBP", "Combined")

ggplot() + 
  geom_line(mape1, mapping=aes(x=Iteration, y=ARIMA, color="ARIMA")) + 
  geom_line(mape1, mapping=aes(x=h, y=ETS, color="ETS")) + 
  geom_line(mape1, mapping=aes(x=h, y=HW, color="HW")) + 
  geom_line(mape1, mapping=aes(x=h, y=FBP, color="FBP")) + 
  geom_line(mape1, mapping=aes(x=h, y=Combined, color="Combined")) +
  ggtitle("Recursive Backtesting, 12 Steps Ahead") + 
  ylab("MAPE") + 
  labs(colour="Model")
```

```{r}
mape2 <- cbind.data.frame(h, arima_roll$V1, ets_roll$V1, 
                          hw_roll$V1, fbp_roll$V1, comb_roll$V1)
colnames(mape2) <- c("Iteration", "ARIMA", "ETS", "HW", "FBP", "Combined")

ggplot() + 
  geom_line(mape2, mapping=aes(x=Iteration, y=ARIMA, color="ARIMA")) + 
  geom_line(mape2, mapping=aes(x=h, y=ETS, color="ETS")) + 
  geom_line(mape2, mapping=aes(x=h, y=HW, color="HW")) + 
  geom_line(mape2, mapping=aes(x=h, y=FBP, color="FBP")) + 
  geom_line(mape2, mapping=aes(x=h, y=Combined, color="Combined")) +
  ggtitle("Rolling Window Backtesting, 12 Steps Ahead") + 
  ylab("MAPE") + 
  labs(colour="Model")
```

```{r, message=F}
model  <- prophet(sales_df)
future <- make_future_dataframe(model, periods=12, 
                                   freq='month')

f_prophet <- predict(model, future)

fbp <- ts(na.remove(f_prophet$yhat[1:(nrow(f_prophet)-12)]), 
          start=1992, frequency=12)
ari <- auto.arima(sales_ts)
ets <- ets(sales_ts)
how <- hw(sales_ts)
com <- (ari[["fitted"]] + ets[["fitted"]] + how[["fitted"]] + fbp) / 4
```

```{r}
res <- sales_ts - com
ggtsdisplay(res, main="Residuals from Combined Model")
```
As you can see, the combined model struggles (as with the other models), to capture the spike in sales in 2020. The ACF and PACF plots of the residuals also show that there is still a small amount of signal that the model could not capture. 

```{r}
Box.test(res, type="Box-Pierce")
Box.test(res, type="Ljung-Box")
```

We can see that, statistically, the residuals from the combined model are white noise. 

```{r}
cusum   <- efp(res ~ 1, type="Rec-CUSUM")
bound_u <- boundary(cusum, alpha=0.05)
bound_l <- -1*boundary(cusum, alpha=0.05)
horizon <- ts(rep(0, times=length(cusum$process)), start=1992, frequency=12)

autoplot(cusum$process) + 
  autolayer(bound_u, color="red") + 
  autolayer(bound_l, color="red") + 
  autolayer(horizon, color="black") +
  ggtitle("Recursive CUSUM Plot") + 
  ylab("Emperical Fluctuation Process") 

```

The CUSUM plot above shows that our model does not break across time. 

```{r}
rr <- recresid(res ~ 1)
x  <- c(1:length(rr))
ggplot() + 
  geom_line(cbind.data.frame(x, rr), mapping=aes(x=x, y=rr)) +
  ylab("Recursive Residuals") + 
  xlab("")
```

Finally, our recursive residuals from the combined model do seem well behaved with exception to the final portion of the time series data which is due to the drastic increase in sales in the beginning of 2020. 

For future work, I may consider using more instances of machine learning models such as k-th Nearest Neighbor (KNN) and neural network autoregression (NNAR) to attempt to obtain more accurate forecasts. 




